{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ATL_Create_Embedding_Files.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!!pip install emoji"
      ],
      "metadata": {
        "id": "GPX5l7EZknH1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8loDujPQPhrq"
      },
      "source": [
        "# Active transfer learning\n",
        "pool is a list of data that it hasn't been labled yet.\n",
        "labeled_data is a list of data that has been labeled"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfJnQ9-7yl8O"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8dAuAY1aTk6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4128eb23-c2c3-4753-f0e4-2030b0932d93"
      },
      "source": [
        "!git clone \"https://github.com/parsafarinnia/Active-Learning-for-Rumor-Identification-on-Social-Media\"\n",
        "DATA_PATH = '/content/Active-Learning-for-Rumor-Identification-on-Social-Media'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'sentiment_new_approach'...\n",
            "remote: Enumerating objects: 60, done.\u001b[K\n",
            "remote: Counting objects: 100% (60/60), done.\u001b[K\n",
            "remote: Compressing objects: 100% (54/54), done.\u001b[K\n",
            "remote: Total 60 (delta 16), reused 33 (delta 4), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (60/60), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0-Xc4Y1mq0U"
      },
      "source": [
        "import tensorflow as tf\n",
        "import torch\n",
        "import os\n",
        "import pickle\n",
        "import pandas as pd\n",
        "!pip install transformers\n",
        "from transformers import BertTokenizer\n",
        "from torch.utils.data import TensorDataset, random_split\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "import numpy as np\n",
        "import time\n",
        "import datetime\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "import json\n",
        "import seaborn as sns\n",
        "from tensorflow import keras\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "!pip install modAL\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWgaL83ZTAMi"
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMhaZEKEPIJ2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcda5369-a6a3-40b4-ca29-8cb06d03465b"
      },
      "source": [
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla K80\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-esnaFdhBBS"
      },
      "source": [
        "# Representation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqfWqiX7gYKI"
      },
      "source": [
        "## TweetBERT (Or BERT)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data"
      ],
      "metadata": {
        "id": "5yGiyxgLDJ_b"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgSfeIiaoeFJ"
      },
      "source": [
        "all_topics = pickle.load( open( \"/content/Active-Learning-for-Rumor-Identification-on-Social-Media/all_topics.p\", \"rb\" ) )\n",
        "data=pd.DataFrame.from_dict(all_topics[0]).T\n",
        "data['topic']=0\n",
        "for i in range(1,9):\n",
        "  topic = pd.DataFrame.from_dict(all_topics[i]).T\n",
        "  if i == 3: print('hi')\n",
        "  topic['topic'] = i\n",
        "  data = pd.concat([data,topic],axis=0)\n",
        "map = {\"rumours\":1,\"non-rumours\":0}\n",
        "data=data.replace({'tag':map})\n",
        "data  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model"
      ],
      "metadata": {
        "id": "GUqBUL88DL1O"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oQ61gE0pnZL"
      },
      "source": [
        "from transformers import AutoModel, AutoTokenizer \n",
        "# Load BERT TWEET tokenizer and model\n",
        "bert_tweet = AutoModel.from_pretrained(\"vinai/bertweet-base\")\n",
        "tokenizer_tweet = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", normalization=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-j1z0rI5FNVL"
      },
      "source": [
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "def tokenizer_func(tokenizer_kind,sentences,labels):\n",
        "  '''\n",
        "  inputs:\n",
        "    tokenizer_kind: is the the tokenizer of choice (normal bert, tweet bert)  \n",
        "    sentences: train , dev, test\n",
        "  outputs:\n",
        "  torchs of \n",
        "    ids\n",
        "    attention_mask\n",
        "    labels\n",
        "  '''\n",
        "  input_ids = []\n",
        "  attention_masks = []\n",
        "\n",
        "  # For every sentence...\n",
        "  for sent in sentences:\n",
        "      # `encode_plus` will:\n",
        "      #   (1) Tokenize the sentence.\n",
        "      #   (2) Prepend the `[CLS]` token to the start.\n",
        "      #   (3) Append the `[SEP]` token to the end.\n",
        "      #   (4) Map tokens to their IDs.\n",
        "      #   (5) Pad or truncate the sentence to `max_length`\n",
        "      #   (6) Create attention masks for [PAD] tokens.\n",
        "      encoded_dict = tokenizer_kind.encode_plus(\n",
        "                          sent,                      # Sentence to encode.\n",
        "                          add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                          max_length = 128,           # Pad & truncate all sentences.\n",
        "                          pad_to_max_length = True,\n",
        "                          return_attention_mask = True,   # Construct attn. masks.\n",
        "                          return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                          truncation=True,\n",
        "                    )\n",
        "      \n",
        "      # Add the encoded sentence to the list.    \n",
        "      input_ids.append(encoded_dict['input_ids'])\n",
        "      \n",
        "      # And its attention mask (simply differentiates padding from non-padding).\n",
        "      attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "  # Convert the lists into tensors.\n",
        "  input_ids = torch.cat(input_ids, dim=0)\n",
        "  attention_masks = torch.cat(attention_masks, dim=0)\n",
        "  labels = torch.tensor(labels)\n",
        "  return input_ids, attention_masks ,labels\n",
        "  # Print sentence 0, now as a list of IDs.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9PO4WiaOFU01",
        "outputId": "4f096479-1165-4210-8b12-7e09986437aa"
      },
      "source": [
        "sentences = data.text.values\n",
        "train_labels = data.tag.values\n",
        "input_ids,attention_masks,labels=tokenizer_func(tokenizer_tweet,sentences,train_labels)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2227: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYB5FSknIx_u"
      },
      "source": [
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "from torch.utils.data import TensorDataset\n",
        "batch_size=128\n",
        "prediction_data = TensorDataset(input_ids, attention_masks, labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSSKkuwzSJI4",
        "outputId": "a2ce9fb5-3b48-4beb-c48f-b9d5c0adf88b"
      },
      "source": [
        "bert_tweet.cuda()\n",
        "bert_tweet.eval()\n",
        "# Tracking variables \n",
        "sentence_feature=[]\n",
        "t1 = time.time()\n",
        "for batch in prediction_dataloader:\n",
        "  t0 = time.time()  \n",
        "  # Add batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "  with torch.no_grad():\n",
        "      outputs = bert_tweet(b_input_ids, attention_mask=b_input_mask)\n",
        "      \n",
        "  sentence_features_slice = outputs[0][:,0,:].cpu().numpy()  \n",
        "  # Store predictions and true labels\n",
        "  sentence_feature.append(sentence_features_slice)\n",
        "  elapsed = format_time(time.time() - t0)\n",
        "  print(\"time elapse:\",elapsed)\n",
        "print(\"full time\",format_time(time.time()-t1))\n",
        "sentence_feature = np.concatenate(sentence_feature, axis=0)\n",
        "print(sentence_feature.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time elapse: 0:00:02\n",
            "time elapse: 0:00:02\n",
            "time elapse: 0:00:02\n",
            "time elapse: 0:00:02\n",
            "time elapse: 0:00:02\n",
            "time elapse: 0:00:02\n",
            "time elapse: 0:00:02\n",
            "time elapse: 0:00:02\n",
            "time elapse: 0:00:02\n",
            "time elapse: 0:00:02\n",
            "time elapse: 0:00:02\n",
            "time elapse: 0:00:02\n",
            "time elapse: 0:00:02\n",
            "time elapse: 0:00:02\n",
            "time elapse: 0:00:02\n",
            "time elapse: 0:00:02\n",
            "time elapse: 0:00:02\n",
            "time elapse: 0:00:02\n",
            "time elapse: 0:00:02\n",
            "time elapse: 0:00:02\n",
            "time elapse: 0:00:02\n",
            "time elapse: 0:00:02\n",
            "time elapse: 0:00:02\n",
            "time elapse: 0:00:02\n",
            "time elapse: 0:00:02\n",
            "time elapse: 0:00:02\n",
            "time elapse: 0:00:02\n",
            "time elapse: 0:00:02\n",
            "time elapse: 0:00:02\n",
            "time elapse: 0:00:02\n",
            "time elapse: 0:00:02\n",
            "time elapse: 0:00:02\n",
            "time elapse: 0:00:02\n",
            "time elapse: 0:00:02\n",
            "time elapse: 0:00:02\n",
            "time elapse: 0:00:02\n",
            "time elapse: 0:00:02\n",
            "time elapse: 0:00:02\n",
            "time elapse: 0:00:02\n",
            "time elapse: 0:00:02\n",
            "time elapse: 0:00:02\n",
            "time elapse: 0:00:02\n",
            "time elapse: 0:00:02\n",
            "time elapse: 0:00:02\n",
            "time elapse: 0:00:02\n",
            "time elapse: 0:00:02\n",
            "time elapse: 0:00:02\n",
            "time elapse: 0:00:02\n",
            "time elapse: 0:00:02\n",
            "time elapse: 0:00:02\n",
            "time elapse: 0:00:00\n",
            "full time 0:01:31\n",
            "(6425, 768)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZCCzuZFGqow"
      },
      "source": [
        "\n",
        "with open('tweet_bert.npy', 'wb') as f:\n",
        "    np.save(f, sentence_feature)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMsMJxBOXffm"
      },
      "source": [
        "## Glove word embeding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3pjS7AaCiO_"
      },
      "source": [
        "### Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VJe6hrBCrC6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bb40608-240e-4d24-d14a-add19dce7269"
      },
      "source": [
        "import nltk\n",
        "# Uncomment to download \"stopwords\"\n",
        "nltk.download(\"stopwords\")\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "import os\n",
        "\n",
        "def text_preprocessing(s):\n",
        "    \"\"\"\n",
        "    - Lowercase the sentence\n",
        "    - Change \"'t\" to \"not\"\n",
        "    - Change \"@name\" to \"Username\"\n",
        "    - Isolate and remove punctuations except \"?\"\n",
        "    - Remove other special characters\n",
        "    - Remove stop words except \"not\" and \"can\"\n",
        "    - Remove trailing whitespace\n",
        "    - change urls to \"Link\"\n",
        "    \"\"\"\n",
        "    s = s.lower()\n",
        "    # Change 't to 'not'\n",
        "    s = re.sub(r\"\\'t\", \" not\", s)\n",
        "    # Change \"@name\" to \"Username\n",
        "    s = re.sub(r'(@.*?)[\\s]', 'username', s)\n",
        "    # Isolate and remove punctuations except '?'\n",
        "    s = re.sub(r'([\\'\\\"\\.\\(\\)\\!\\?\\\\\\/\\,])', r' \\1 ', s)\n",
        "    s = re.sub(r'[^\\w\\s\\?]', ' ', s)\n",
        "    # Remove some special characters\n",
        "    s = re.sub(r'([\\;\\:\\|•«\\n])', ' ', s)\n",
        "    # Remove stopwords except 'not' and 'can'\n",
        "    s = \" \".join([word for word in s.split()\n",
        "                  if word not in stopwords.words('english')\n",
        "                  or word in ['not', 'can']])\n",
        "    # Remove trailing whitespace\n",
        "    s = re.sub(r'\\s+', ' ', s).strip()\n",
        "    # change urls to \"Link\"\n",
        "    s = re.sub(r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\",'link',s)\n",
        "    \n",
        "    return s"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jknz7wXACuzy"
      },
      "source": [
        "import numpy as np\n",
        "sentences = np.array([text_preprocessing(text) for text in data.text.values  ])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BsqrqPZpT9bp"
      },
      "source": [
        "### Glove\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget 'https://nlp.stanford.edu/data/glove.twitter.27B.zip' "
      ],
      "metadata": {
        "id": "SiwPnlZTB5ZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYotFrQnaFr4"
      },
      "source": [
        "!unzip \"glove.twitter.27B.zip\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kv59vXazjpqB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6868a69b-bdef-4f5b-b2ab-7cc10a6b8a62"
      },
      "source": [
        "def loadGloveModel(File):\n",
        "    print(\"Loading Glove Model\")\n",
        "    f = open(File,'r')\n",
        "    gloveModel = {}\n",
        "    for line in f:\n",
        "        splitLines = line.split()\n",
        "        word = splitLines[0]\n",
        "        wordEmbedding = np.array([float(value) for value in splitLines[1:]])\n",
        "        gloveModel[word] = wordEmbedding\n",
        "    print(len(gloveModel),\" words loaded!\")\n",
        "    return gloveModel\n",
        "glove = loadGloveModel('glove.twitter.27B.50d.txt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading Glove Model\n",
            "1193514  words loaded!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UePgEF3o8zY"
      },
      "source": [
        "\n",
        "data_train = np.zeros(shape=(len(sentences),50),dtype=np.float)\n",
        "\n",
        "for i in range(len(sentences)):\n",
        "  tweet = sentences[i]\n",
        "  tweet = tweet.split(' ')\n",
        "  glove_word_count = 1\n",
        "  for word in tweet:\n",
        "    if word.lower() in glove:\n",
        "      glove_word_count +=1\n",
        "      data_train[i,:]+= glove[word]\n",
        "  data_train[i,:]=data_train[i,:]/(glove_word_count)\n",
        "features=data_train\n",
        "#representation Glove"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwDjsrB3lBRA"
      },
      "source": [
        "sentence_feature_GLOVE=features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p54Zd_3dvLRY",
        "outputId": "e1e64489-37dd-4c02-be97-441be604da84"
      },
      "source": [
        "sentence_feature_GLOVE.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6425, 100)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBghjd46vsCu"
      },
      "source": [
        "save_reesult(sentence_feature_GLOVE,'GLOVE50.np')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}